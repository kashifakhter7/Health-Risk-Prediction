# -*- coding: utf-8 -*-
"""ATP_Binding_MLP_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1354Q6yVshnSSO5cJ7VcSpF08nCWhC3N4

# ATP-Binding Residue Prediction using ProtBERT + MLP
"""

!pip install transformers torch scikit-learn biopython

from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("atp_binding_dataset.csv")
df = df.dropna(how='all')
df = df[:-405]  # trimming as per original code

# Load ProtBERT
tokenizer = BertTokenizer.from_pretrained("Rostlab/prot_bert", do_lower_case=False)
model = BertModel.from_pretrained("Rostlab/prot_bert")
model.eval()

# Embedding function
def embed_sequence(seq):
    seq = ' '.join(list(seq))
    inputs = tokenizer(seq, return_tensors="pt")
    with torch.no_grad():
        output = model(**inputs)
    embeddings = output.last_hidden_state[0]
    return embeddings

# Generate embeddings and labels
X = []
Y = []
for i, row in tqdm(df.iterrows(), total=len(df)):
    seq = row['sequence']
    label = list(map(int, row['labels'].split(',')))
    emb = embed_sequence(seq)
    emb = emb[1:len(label)+1]
    if emb.shape[0] != len(label):
        continue
    X.append(emb)
    Y.append(torch.tensor(label))

X_all = torch.cat(X, dim=0)
Y_all = torch.cat(Y, dim=0)

# Proper stratified train-test split
X_np = X_all.numpy()
Y_np = Y_all.numpy()

X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(
    X_np, Y_np, test_size=0.2, stratify=Y_np, random_state=42)

X_train = torch.tensor(X_train_np).float()
X_test = torch.tensor(X_test_np).float()
y_train = torch.tensor(y_train_np).float()
y_test = torch.tensor(y_test_np).float()

# Define MLP Classifier
class MLPClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=512):
        super(MLPClassifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.fc(x)

# Model setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
mlp = MLPClassifier().to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(mlp.parameters(), lr=1e-4)

# Training loop
batch_size = 128
epochs = 5

for epoch in range(epochs):
    mlp.train()
    total_loss = 0
    for i in range(0, len(X_train), batch_size):
        xb = X_train[i:i+batch_size].to(device)
        yb = y_train[i:i+batch_size].float().to(device)
        pred = mlp(xb).squeeze()
        loss = criterion(pred, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

# Evaluation
mlp.eval()
with torch.no_grad():
    y_pred = mlp(X_test.to(device)).squeeze().cpu().numpy()
    y_true = y_test.numpy()

y_bin = (y_pred > 0.5).astype(int)

print("\nClassification Report:")
print(classification_report(y_true, y_bin))

try:
    auc_score = roc_auc_score(y_true, y_pred)
    print(f"\nROC AUC Score: {auc_score:.4f}")
except ValueError as e:
    print("\nROC AUC Score could not be calculated:", e)

print("\nClass distribution in test set:")
print(np.unique(y_true, return_counts=True))

# Confusion matrix
cm = confusion_matrix(y_true, y_bin)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()
