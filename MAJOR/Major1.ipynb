{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VhnsMgJnv8wp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy dataset with sequence and structure features\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, seq_length=128, feature_dim=64):\n",
        "        self.seq_features = np.random.rand(num_samples, seq_length, feature_dim)\n",
        "        self.struct_features = np.random.rand(num_samples, seq_length, feature_dim)\n",
        "        self.labels = np.random.randint(0, 2, (num_samples, seq_length))  # Binary labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.seq_features[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.struct_features[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        )"
      ],
      "metadata": {
        "id": "GVNLxx-5wKkt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Co-Attention Mechanism for Multi-View Learning\n",
        "class CoAttention(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(CoAttention, self).__init__()\n",
        "        self.seq_fc = nn.Linear(feature_dim, feature_dim)\n",
        "        self.struct_fc = nn.Linear(feature_dim, feature_dim)\n",
        "        self.attn = nn.Linear(feature_dim, 1)\n",
        "\n",
        "    def forward(self, seq_features, struct_features):\n",
        "        seq_proj = torch.tanh(self.seq_fc(seq_features))\n",
        "        struct_proj = torch.tanh(self.struct_fc(struct_features))\n",
        "        attn_weights = torch.softmax(self.attn(seq_proj + struct_proj), dim=1)\n",
        "        co_attended = attn_weights * seq_features + (1 - attn_weights) * struct_features\n",
        "        return co_attended\n"
      ],
      "metadata": {
        "id": "JMZzkssYwRFF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Model with Multi-View Learning\n",
        "class ATPBindingPredictor(nn.Module):\n",
        "    def __init__(self, seq_length=128, feature_dim=64, hidden_dim=128):\n",
        "        super(ATPBindingPredictor, self).__init__()\n",
        "        self.co_attention = CoAttention(feature_dim)\n",
        "        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, seq_features, struct_features):\n",
        "        co_attended = self.co_attention(seq_features, struct_features)\n",
        "        lstm_out, _ = self.lstm(co_attended)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "BxE_x3PawXQ3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for seq_features, struct_features, labels in train_loader:\n",
        "            seq_features, struct_features, labels = seq_features.to(device), struct_features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(seq_features, struct_features)\n",
        "            loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "Fy65qn5Ywfy1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = ProteinDataset()\n",
        "    train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "    model = ATPBindingPredictor()\n",
        "    train_model(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEsKwj5Kwn3b",
        "outputId": "d18268d2-47fb-4f61-a77d-2e72a340963e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zx29jrvwomu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}